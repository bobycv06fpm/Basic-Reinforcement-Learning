#==============================================================================
# This code is modelled for a simple MAB problem where the distribution of the 
# rewards are assumed to be normally distributed. The number of player is only
# one.
#==============================================================================

# Importing the useful libraries
import numpy as np
import random as rand
import matplotlib.pyplot as plot 

#==============================================================================

# Defining the function to pick an arm(RV form the distribution) randomly
def arm_reward_samples(mu,sd):
    RV=np.random.normal(mu,sd);
    return RV;

#==============================================================================

# Taking the momdel parameters as entry here
num_rounds = 300;     # No. of rounds to be played / Time till which we survey

dev_factor = 0.5;            # Deviation of each reward ddistribution from its mean
num_arms = 3;         # Number of restaurants / arms

#==============================================================================

# Deciding the mean and variance of the arm distributions  and then getting the 
# theoretical already known optimal regret as well

# Getting the Mean and SD of distribution of each restaurant / arm
dev_vals=[];
mu_vals=[];
for i in range(0,num_restaurants):
    mu_vals.append(3*(i+1));
    dev_vals.append(mu_vals[i]*dev_factor);
    
    
optimal_mean_reward = max(mu_vals)*num_days;     # Maximum reward possible


print("\nThe mean of the arm distributions are - ",mu_vals,"\n");
print("The deviation of the arm distributions are - ",dev_vals,"\n");  

#==============================================================================

# Algorithm - Exploit Only
# Defining the function that calculates the rewards for exploit only algorithm
def only_exploitation (num_of_rounds):   
    reward=[];
    reward_for_all_arms_explored_once=[];
    
    # Collecting regret samples from all arms once
    for i in range(0,num_arms):
        a=arm_reward_samples(mu_vals[i],dev_vals[i]);
        reward_for_all_arms_explored_once.append(a);
    
    del i
    # Deciding the arm to be exploited based on just one exploration    
    index=np.argmax(reward_for_all_arms_explored_once);
    
    # Exploting the arm which was earlier decided to be optimal
    for i in range(0,num_of_rounds):
        reward.append(arm_reward_samples(mu_vals[index],dev_vals[index]));
    
    del i
    return sum(reward);

#==============================================================================

# Calling the Algorithm and estimating the average regret under it

# Calculating the mean regret for exploit only algorithm
exploit_alone_rewards=[];
for j in range(0,1000):
    r2=only_exploitation(num_rounds);
    exploit_alone_rewards.append(r2);
    
mean_of_exploit_alone_regret=optimal_mean_reward - np.mean(exploit_alone_rewards);
# moetar = "M"ean  "O"f  "E"xploi"T"  "A"lone  "R"egret
moetar=mean_of_exploit_alone_regret/optimal_mean_reward;
print("The regret for Exploit only policy is : ",moetar);

del j